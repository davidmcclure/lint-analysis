Last week I took a look at some of the "cohorts" that were produced by a hierarchical cluster of the distributions of different words across narrative time - groups of words that tend to rise and fall together across the plot, when averaged out across thousands of novels. One side-effect of this kind of exploratory approach, I think, is that there's a tendency to focus on the more semantically "focused" signals that are easy to map back onto the experience of actually reading individual novels. For example, it's not hard to reason about what's going on with cluster 37 (pistol, bullet, gun) or 139 (student, students, school). Which, as Ted and Scott pointed out on Twitter, might tell us more about the presence of different genres in the corpus than about "narrative," in any kind of general sense. But, what to make of something like cluster 10? This seems more muddled, and, notably, includes a number of stopwords - "a," "an," "or," "than," "these" - all of which made it into the initial cut of the 1,000 most narratologically "uneven" or "non-uniform" words.

[37, 139, 10]

Which, it turns out, isn't a fluke. These kinds of function words - any one of which can show up tens or hundreds of millions of times across 27,000 novels, and collectively represent a large slice of the 2.5 billion words in the corpus - tend to have *very* strong trends across narrative time. In fact, stronger than almost anything else in the dictionary. Take a look again at this graph from a couple weeks ago, which plots the the non-uniformity of each word as a function of its frequency:

[]

The Y-axis is just the simple statistical variance of the word's frequency across each percentile of the novel, which gives a crude measure of "unevenness," the degree to which a word isn't a flat line, tends to show up in some regions of the narrative more than others. The blue line represents the null hypothesis, basically - the amount of variance that we'd expect under the uniform distribution, if everything were just random noise. By dividing the observed variance by this theoretical baseline, we can get a simple score for the "unevenness" of the word, how many orders-of-magnitude it is above what it would be if it were flat across the text.

Before, I focused on the fact that almost all of the words fall above this line, which corresponds to the fact that almost all words are significantly different from the uniform distribution, and have some kind of "trend" across the novel. But, that's not all. It's also clear that the slope of the data is higher than the slope of the line, that words rise higher above the line on the right side of the graph than on the left - on the left side of the graph, the highest-scoring words sit about 2 orders of magnitude above the baseline; at the right side, this rises to about 3. Words seem to become *more* uneven as they become more frequent, even after adjusting for the expected correlation between frequency and variance.

Indeed, when we use this metric to skim off a set of the most non-uniform words, we end up getting a large majority of the most frequent words, and a much smaller slice of the less frequent words. For example, if we take the 200 highest-scoring words, we get 60 out of the 100 most frequent words in the corpus:

[200 words, bolded]

What to make of this? It's kind of baffling, and runs totally against what I expected at the start. I assumed that the function words would be basically flat, maybe with some very slight fluctuation, since I don't really think of them as having any kind of narratological valence / affinity that would cause them to attach to beginnings / middles / ends in the way that things like "death" and "marriage" do. I thought they'd probably be *negative* examples of what I was looking for, almost - words that have to show up everywhere more or less evenly, almost out of necessity, the connective tissue of the language that's needed everywhere equally. (Though I also remembered Matt Jockers' finding from Macroanalysis that the word "the" rises and falls across historical time, and a little voice in my head wondered if there might be similar effects across narrative time.)

Usually, when something correlates with frequency like this, it feels like a red flag, the worry being that you're somehow just reproducing the fact that frequent words are frequent and infrequent words are infrequent. As a sanity check, I re-ran the exact same feature extraction job on the corpus, but this time, before pulling out the percentile-sliced word counts for each text, I randomly shuffled the words to destroy any kind of narratological ordering. Sure enough, with this, the variances clamp right onto the expected line:

[random variances]

So - as far as I can make it out, I think there is actually some meaningful way in which the highest frequency words are the most skewed across the narrative, the most uneven, the most narratologically charged? This seemed totally bizarre to me at first, then I convinced myself that it wasn't actually that weird, but now I'm back to thinking it's bizarre. But, even now, I'm not at all sure that my expectations are / were reasonable. Am I wrong to be surprised by this? The effect is so strong, it makes me wonder - is it somehow tautologically true, is there some kind of fundamental linguistic / literary / information-theoretic pressure that would make it impossible for this not to be the case, in some way? Or is it just obvious that this would be the case?

Part of the explanation, I think, is that this question of whether a word is narratologically "uneven" is actually less cut-and-dry than it seemed to be at first, and it gets caught up in interesting ways with the overall frequency of the word. For example, take "gun" and "a":

[a, gun histograms]

"Gun" appears 174,286 times; "a" appears 44,510,387 times, about 255 times as often. Which of these is more "surprising"? At a kind of visual / intuitive level, "gun" obviously has the more dramatic trend - a huge spike around the 95% marker, the moment of the climax, where it literally doubles in volume relative to the baseline across the first half of the narrative. Indeed, if you convert them into probability density functions - throwing out any information about about the overall frequencies - and then compare them to a uniform distribution using pretty much any goodness-of-fit test or distance metric, "gun" will always score higher by a large margin. Just using the Euclidean distance, like I did last week for the hierarchical cluster - "gun" has a distance of XX from the uniform distribution, whereas "a" is just YY.

[a, gun series]